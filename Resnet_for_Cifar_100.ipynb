{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWHczmrl9sZh"
      },
      "source": [
        "# Part I. Preparation\n",
        "\n",
        "First, we load the CIFAR-100 dataset. This might take a couple minutes the first time you do it, but the files should stay cached after that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tRz7e1jG9sZh"
      },
      "outputs": [],
      "source": [
        "# Add official website of pytorch\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import sampler\n",
        "\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as T\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxYnq0EF9sZh",
        "outputId": "0d87fbc8-9926-4ba5-980f-4575d6072721"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169M/169M [00:38<00:00, 4.43MB/s]\n"
          ]
        }
      ],
      "source": [
        "NUM_TRAIN = 49000\n",
        "batch_size= 64\n",
        "\n",
        "# The torchvision.transforms package provides tools for preprocessing data\n",
        "# and for performing data augmentation; here we set up a transform to\n",
        "# preprocess the data by subtracting the mean RGB value and dividing by the\n",
        "# standard deviation of each RGB value; we've hardcoded the mean and std.\n",
        "\n",
        "#===========================================================================#\n",
        "# You should try changing the transform for the training data to include    #\n",
        "# data augmentation such as RandomCrop and HorizontalFlip                    #\n",
        "# when running the final part of the notebook where you have to achieve     #\n",
        "# as high accuracy as possible on CIFAR-100.                                #\n",
        "# Of course you will have to re-run this block for the effect to take place #\n",
        "#===========================================================================#\n",
        "train_transform = transform = T.Compose([\n",
        "                T.ToTensor(),\n",
        "                T.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
        "            ])\n",
        "\n",
        "# We set up a Dataset object for each split (train / val / test); Datasets load\n",
        "# training examples one at a time, so we wrap each Dataset in a DataLoader which\n",
        "# iterates through the Dataset and forms minibatches. We divide the CIFAR-100\n",
        "# training set into train and val sets by passing a Sampler object to the\n",
        "# DataLoader telling how it should sample from the underlying Dataset.\n",
        "cifar100_train = dset.CIFAR100('./datasets/cifar100', train=True, download=True,\n",
        "                             transform=train_transform)\n",
        "loader_train = DataLoader(cifar100_train, batch_size=batch_size, num_workers=2,\n",
        "                          sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
        "\n",
        "cifar100_val = dset.CIFAR100('./datasets/cifar100', train=True, download=True,\n",
        "                           transform=transform)\n",
        "loader_val = DataLoader(cifar100_val, batch_size=batch_size, num_workers=2,\n",
        "                        sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, 50000)))\n",
        "\n",
        "cifar100_test = dset.CIFAR100('./datasets/cifar100', train=False, download=True,\n",
        "                            transform=transform)\n",
        "loader_test = DataLoader(cifar100_test, batch_size=batch_size, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwm508C59sZi"
      },
      "source": [
        "You have an option to **use GPU by setting the flag to True below** (recommended). It is not necessary to use GPU for this assignment. Note that if your computer does not have CUDA enabled, `torch.cuda.is_available()` will return False and this notebook will fallback to CPU mode.\n",
        "**You can run on GPU on datahub.**\n",
        "\n",
        "The global variables `dtype` and `device` will control the data types throughout this assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlL7rlyv9sZj",
        "outputId": "79af5b70-265d-4281-e6ce-949c2e7b4a90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using device: cuda\n"
          ]
        }
      ],
      "source": [
        "USE_GPU = True\n",
        "num_class = 100\n",
        "dtype = torch.float32 # we will be using float throughout this tutorial\n",
        "\n",
        "if USE_GPU and torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "# Constant to control how frequently we print train loss\n",
        "\n",
        "\n",
        "print('using device:', device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbQiGzWG9sZj",
        "outputId": "90185a29-852e-4e65-db3e-0b4dede299b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before flattening:  tensor([[[[ 0,  1],\n",
            "          [ 2,  3],\n",
            "          [ 4,  5]]],\n",
            "\n",
            "\n",
            "        [[[ 6,  7],\n",
            "          [ 8,  9],\n",
            "          [10, 11]]]])\n",
            "After flattening:  tensor([[ 0,  1,  2,  3,  4,  5],\n",
            "        [ 6,  7,  8,  9, 10, 11]])\n"
          ]
        }
      ],
      "source": [
        "def flatten(x):\n",
        "    N = x.shape[0] # read in N, C, H, W\n",
        "    return x.view(N, -1)  # \"flatten\" the C * H * W values into a single vector per image\n",
        "\n",
        "def test_flatten():\n",
        "    x = torch.arange(12).view(2, 1, 3, 2)\n",
        "    print('Before flattening: ', x)\n",
        "    print('After flattening: ', flatten(x))\n",
        "\n",
        "test_flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzHo1YD69sZl",
        "outputId": "4e64c354-db67-4da9-9619-965a2220d8c4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.5881, -0.9331, -0.5931, -1.0455, -0.0518],\n",
              "        [ 1.1819,  0.6901, -0.2150, -0.5044, -0.1865],\n",
              "        [-0.4173,  0.2111,  0.0749, -0.3254,  1.4280]], device='cuda:0',\n",
              "       requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "def random_weight(shape):\n",
        "    \"\"\"\n",
        "    Create random Tensors for weights; setting requires_grad=True means that we\n",
        "    want to compute gradients for these Tensors during the backward pass.\n",
        "    We use Kaiming normalization: sqrt(2 / fan_in)\n",
        "    \"\"\"\n",
        "    if len(shape) == 2:  # FC weight\n",
        "        fan_in = shape[0]\n",
        "    else:\n",
        "        fan_in = np.prod(shape[1:]) # conv weight [out_channel, in_channel, kH, kW]\n",
        "    # randn is standard normal distribution generator.\n",
        "    w = torch.randn(shape, device=device, dtype=dtype) * np.sqrt(2. / fan_in)\n",
        "    w.requires_grad = True\n",
        "    return w\n",
        "\n",
        "def zero_weight(shape):\n",
        "    return torch.zeros(shape, device=device, dtype=dtype, requires_grad=True)\n",
        "\n",
        "# create a weight of shape [3 x 5]\n",
        "# you should see the type `torch.cuda.FloatTensor` if you use GPU.\n",
        "# Otherwise it should be `torch.FloatTensor`\n",
        "random_weight((3, 5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cz1LnyVK9sZm"
      },
      "source": [
        "### Check Accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_xquRmc9sZo"
      },
      "source": [
        "### Module API: Check Accuracy\n",
        "Given the validation or test set, we can check the classification accuracy of a neural network.\n",
        "\n",
        "This version is slightly different from the one in part II. You don't manually pass in the parameters anymore."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "18qWWK3R9sZo"
      },
      "outputs": [],
      "source": [
        "def check_accuracy_part34(loader, model):\n",
        "    if loader.dataset.train:\n",
        "        print('Checking accuracy on validation set')\n",
        "    else:\n",
        "        print('Checking accuracy on test set')\n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    model.eval()  # set model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
        "            y = y.to(device=device, dtype=torch.long)\n",
        "            scores = model(x)\n",
        "            _, preds = scores.max(1)\n",
        "            num_correct += (preds == y).sum()\n",
        "            num_samples += preds.size(0)\n",
        "        acc = float(num_correct) / num_samples\n",
        "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sk9E0pW9sZo"
      },
      "source": [
        "### Module API: Training Loop\n",
        "We also use a slightly different training loop. Rather than updating the values of the weights ourselves, we use an Optimizer object from the `torch.optim` package, which abstract the notion of an optimization algorithm and provides implementations of most of the algorithms commonly used to optimize neural networks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "rAWGcdvL9sZo"
      },
      "outputs": [],
      "source": [
        "def train_part34(model, optimizer, epochs=1):\n",
        "    \"\"\"\n",
        "    Train a model on CIFAR-10 using the PyTorch Module API.\n",
        "\n",
        "    Inputs:\n",
        "    - model: A PyTorch Module giving the model to train.\n",
        "    - optimizer: An Optimizer object we will use to train the model\n",
        "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
        "\n",
        "    Returns: The accuracy of the model\n",
        "    \"\"\"\n",
        "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
        "    for e in range(epochs):\n",
        "        for t, (x, y) in enumerate(loader_train):\n",
        "            model.train()  # put model to training mode\n",
        "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
        "            y = y.to(device=device, dtype=torch.long)\n",
        "\n",
        "            scores = model(x)\n",
        "            loss = F.cross_entropy(scores, y)\n",
        "\n",
        "            # Zero out all of the gradients for the variables which the optimizer\n",
        "            # will update.\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # This is the backwards pass: compute the gradient of the loss with\n",
        "            # respect to each  parameter of the model.\n",
        "            loss.backward()\n",
        "\n",
        "            # Actually update the parameters of the model using the gradients\n",
        "            # computed by the backwards pass.\n",
        "            optimizer.step()\n",
        "\n",
        "        print('Epoch %d, Iteration %d, loss = %.4f' % (e, t + 1, loss.item()))\n",
        "        check_accuracy_part34(loader_val, model)\n",
        "        print()\n",
        "\n",
        "    return check_accuracy_part34(loader_val, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqEkvqJT9sZq"
      },
      "source": [
        "# Resnet10 Implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Y_ZNttCNln65"
      },
      "outputs": [],
      "source": [
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, batch_use = False):\n",
        "        super(BasicBlock, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, (3,3), stride=1, padding=1, bias = True)\n",
        "        # only use Batchnorm in part 2\n",
        "        self.BatchNorm1 = nn.BatchNorm2d(out_channels) if batch_use else nn.Identity()\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, (3,3), stride=1, padding=1, bias = True)\n",
        "        self.BatchNorm2 = nn.BatchNorm2d(out_channels) if batch_use else nn.Identity()\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.shortcut = nn.Conv2d(in_channels, out_channels, (1,1), stride=1, padding=0, bias = True)\n",
        "        if in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, (1,1), stride=1, padding=0, bias = True),\n",
        "                # nn.ReLU(inplace=True)\n",
        "\n",
        "            )\n",
        "            self.shortcut[0].weight.data = random_weight((out_channels, in_channels, 1, 1))\n",
        "            self.shortcut[0].bias.data = zero_weight((out_channels,))\n",
        "\n",
        "        self.conv1.weight.data = random_weight((out_channels, in_channels, 3, 3))\n",
        "        self.conv1.bias.data = zero_weight((out_channels,))\n",
        "        self.conv2.weight.data = random_weight((out_channels, out_channels, 3, 3))\n",
        "        self.conv2.bias.data = zero_weight((out_channels,))\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x if self.shortcut is None else self.shortcut(x)\n",
        "        out = self.conv1(x)\n",
        "        # only use Batchnorm in part 2\n",
        "        out = self.BatchNorm1(out)\n",
        "\n",
        "        out = self.relu1(out)\n",
        "        out = self.conv2(out)\n",
        "        # only use Batchnorm in part 2\n",
        "        out = self.BatchNorm2(out)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu2(out)\n",
        "        return out\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, num_classes = 100, batch_use = False):\n",
        "        super(ResNet, self).__init__()\n",
        "\n",
        "        # conv1\n",
        "        self.conv1 = nn.Conv2d(in_channels, 64, (7,7), stride=2, padding=1, bias = True)\n",
        "        # only use Batchnorm in part 2\n",
        "        self.BatchNorm = nn.BatchNorm2d(64) if batch_use else nn.Identity()\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d((3,3), stride=2, padding=1)\n",
        "\n",
        "        # conv2_x\n",
        "        self.conv2_x = BasicBlock(64, 128, batch_use)\n",
        "\n",
        "        # conv3_3\n",
        "        self.conv3_x = BasicBlock(128, 256, batch_use)\n",
        "\n",
        "        # conv4_x\n",
        "        self.conv4_x = BasicBlock(256, 512, batch_use)\n",
        "\n",
        "        # conv5_x\n",
        "        self.conv5_x = BasicBlock(512, 512,batch_use)\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        # fc\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "        self.conv1.weight.data = random_weight((64, in_channels, 7, 7))\n",
        "        self.conv1.bias.data = zero_weight((64,))\n",
        "        self.fc.weight.data = random_weight((num_classes, 512))\n",
        "        self.fc.bias.data = zero_weight((num_classes,))\n",
        "    def forward(self,x):\n",
        "        x = self.conv1(x)\n",
        "        # only use Batchnorm in part 2\n",
        "        x = self.BatchNorm(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.conv2_x(x)\n",
        "        x = self.conv3_x(x)\n",
        "        x = self.conv4_x(x)\n",
        "        x = self.conv5_x(x)\n",
        "        x = self.avg_pool(x)\n",
        "        x = flatten(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfLJ7e2p9sZq",
        "outputId": "22c8fca7-92ff-40bb-d064-26ab21d3a655"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Iteration 766, loss = 3.9901\n",
            "Checking accuracy on validation set\n",
            "Got 123 / 1000 correct (12.30)\n",
            "\n",
            "Epoch 1, Iteration 766, loss = 2.8448\n",
            "Checking accuracy on validation set\n",
            "Got 229 / 1000 correct (22.90)\n",
            "\n",
            "Epoch 2, Iteration 766, loss = 2.1132\n",
            "Checking accuracy on validation set\n",
            "Got 294 / 1000 correct (29.40)\n",
            "\n",
            "Epoch 3, Iteration 766, loss = 2.8710\n",
            "Checking accuracy on validation set\n",
            "Got 337 / 1000 correct (33.70)\n",
            "\n",
            "Epoch 4, Iteration 766, loss = 2.1469\n",
            "Checking accuracy on validation set\n",
            "Got 360 / 1000 correct (36.00)\n",
            "\n",
            "Epoch 5, Iteration 766, loss = 1.8363\n",
            "Checking accuracy on validation set\n",
            "Got 373 / 1000 correct (37.30)\n",
            "\n",
            "Epoch 6, Iteration 766, loss = 1.7559\n",
            "Checking accuracy on validation set\n",
            "Got 409 / 1000 correct (40.90)\n",
            "\n",
            "Epoch 7, Iteration 766, loss = 1.4281\n",
            "Checking accuracy on validation set\n",
            "Got 408 / 1000 correct (40.80)\n",
            "\n",
            "Epoch 8, Iteration 766, loss = 1.2865\n",
            "Checking accuracy on validation set\n",
            "Got 405 / 1000 correct (40.50)\n",
            "\n",
            "Epoch 9, Iteration 766, loss = 0.8897\n",
            "Checking accuracy on validation set\n",
            "Got 419 / 1000 correct (41.90)\n",
            "\n",
            "Epoch 10, Iteration 766, loss = 0.8231\n",
            "Checking accuracy on validation set\n",
            "Got 375 / 1000 correct (37.50)\n",
            "\n",
            "Epoch 11, Iteration 766, loss = 0.8743\n",
            "Checking accuracy on validation set\n",
            "Got 407 / 1000 correct (40.70)\n",
            "\n",
            "Epoch 12, Iteration 766, loss = 0.4835\n",
            "Checking accuracy on validation set\n",
            "Got 384 / 1000 correct (38.40)\n",
            "\n",
            "Epoch 13, Iteration 766, loss = 0.6389\n",
            "Checking accuracy on validation set\n",
            "Got 400 / 1000 correct (40.00)\n",
            "\n",
            "Epoch 14, Iteration 766, loss = 0.3207\n",
            "Checking accuracy on validation set\n",
            "Got 391 / 1000 correct (39.10)\n",
            "\n",
            "Epoch 15, Iteration 766, loss = 0.7507\n",
            "Checking accuracy on validation set\n",
            "Got 388 / 1000 correct (38.80)\n",
            "\n",
            "Epoch 16, Iteration 766, loss = 0.4509\n",
            "Checking accuracy on validation set\n",
            "Got 402 / 1000 correct (40.20)\n",
            "\n",
            "Epoch 17, Iteration 766, loss = 0.6251\n",
            "Checking accuracy on validation set\n",
            "Got 390 / 1000 correct (39.00)\n",
            "\n",
            "Epoch 18, Iteration 766, loss = 0.3037\n",
            "Checking accuracy on validation set\n",
            "Got 398 / 1000 correct (39.80)\n",
            "\n",
            "Epoch 19, Iteration 766, loss = 0.2842\n",
            "Checking accuracy on validation set\n",
            "Got 388 / 1000 correct (38.80)\n",
            "\n",
            "Epoch 20, Iteration 766, loss = 0.2196\n",
            "Checking accuracy on validation set\n",
            "Got 388 / 1000 correct (38.80)\n",
            "\n",
            "Epoch 21, Iteration 766, loss = 0.1802\n",
            "Checking accuracy on validation set\n",
            "Got 398 / 1000 correct (39.80)\n",
            "\n",
            "Epoch 22, Iteration 766, loss = 0.4565\n",
            "Checking accuracy on validation set\n",
            "Got 349 / 1000 correct (34.90)\n",
            "\n",
            "Epoch 23, Iteration 766, loss = 0.4634\n",
            "Checking accuracy on validation set\n",
            "Got 377 / 1000 correct (37.70)\n",
            "\n",
            "Epoch 24, Iteration 766, loss = 0.3591\n",
            "Checking accuracy on validation set\n",
            "Got 369 / 1000 correct (36.90)\n",
            "\n",
            "Epoch 25, Iteration 766, loss = 0.2775\n",
            "Checking accuracy on validation set\n",
            "Got 390 / 1000 correct (39.00)\n",
            "\n",
            "Epoch 26, Iteration 766, loss = 0.4881\n",
            "Checking accuracy on validation set\n",
            "Got 387 / 1000 correct (38.70)\n",
            "\n",
            "Epoch 27, Iteration 766, loss = 0.0543\n",
            "Checking accuracy on validation set\n",
            "Got 365 / 1000 correct (36.50)\n",
            "\n",
            "Epoch 28, Iteration 766, loss = 0.4607\n",
            "Checking accuracy on validation set\n",
            "Got 375 / 1000 correct (37.50)\n",
            "\n",
            "Epoch 29, Iteration 766, loss = 0.3864\n",
            "Checking accuracy on validation set\n",
            "Got 380 / 1000 correct (38.00)\n",
            "\n",
            "Epoch 30, Iteration 766, loss = 0.2862\n",
            "Checking accuracy on validation set\n",
            "Got 367 / 1000 correct (36.70)\n",
            "\n",
            "Epoch 31, Iteration 766, loss = 0.5164\n",
            "Checking accuracy on validation set\n",
            "Got 376 / 1000 correct (37.60)\n",
            "\n",
            "Epoch 32, Iteration 766, loss = 0.1606\n",
            "Checking accuracy on validation set\n",
            "Got 393 / 1000 correct (39.30)\n",
            "\n",
            "Epoch 33, Iteration 766, loss = 0.3133\n",
            "Checking accuracy on validation set\n",
            "Got 373 / 1000 correct (37.30)\n",
            "\n",
            "Epoch 34, Iteration 766, loss = 0.4223\n",
            "Checking accuracy on validation set\n",
            "Got 362 / 1000 correct (36.20)\n",
            "\n",
            "Epoch 35, Iteration 766, loss = 0.3002\n",
            "Checking accuracy on validation set\n",
            "Got 366 / 1000 correct (36.60)\n",
            "\n",
            "Epoch 36, Iteration 766, loss = 0.5399\n",
            "Checking accuracy on validation set\n",
            "Got 393 / 1000 correct (39.30)\n",
            "\n",
            "Epoch 37, Iteration 766, loss = 0.5856\n",
            "Checking accuracy on validation set\n",
            "Got 371 / 1000 correct (37.10)\n",
            "\n",
            "Epoch 38, Iteration 766, loss = 0.8282\n",
            "Checking accuracy on validation set\n",
            "Got 370 / 1000 correct (37.00)\n",
            "\n",
            "Epoch 39, Iteration 766, loss = 0.2198\n",
            "Checking accuracy on validation set\n",
            "Got 377 / 1000 correct (37.70)\n",
            "\n",
            "Epoch 40, Iteration 766, loss = 0.3509\n",
            "Checking accuracy on validation set\n",
            "Got 400 / 1000 correct (40.00)\n",
            "\n",
            "Epoch 41, Iteration 766, loss = 0.3418\n",
            "Checking accuracy on validation set\n",
            "Got 389 / 1000 correct (38.90)\n",
            "\n",
            "Epoch 42, Iteration 766, loss = 0.5607\n",
            "Checking accuracy on validation set\n",
            "Got 389 / 1000 correct (38.90)\n",
            "\n",
            "Epoch 43, Iteration 766, loss = 0.4990\n",
            "Checking accuracy on validation set\n",
            "Got 360 / 1000 correct (36.00)\n",
            "\n",
            "Epoch 44, Iteration 766, loss = 0.8587\n",
            "Checking accuracy on validation set\n",
            "Got 373 / 1000 correct (37.30)\n",
            "\n",
            "Epoch 45, Iteration 766, loss = 0.6154\n",
            "Checking accuracy on validation set\n",
            "Got 367 / 1000 correct (36.70)\n",
            "\n",
            "Epoch 46, Iteration 766, loss = 0.1084\n",
            "Checking accuracy on validation set\n",
            "Got 350 / 1000 correct (35.00)\n",
            "\n",
            "Epoch 47, Iteration 766, loss = 0.2443\n",
            "Checking accuracy on validation set\n",
            "Got 378 / 1000 correct (37.80)\n",
            "\n",
            "Epoch 48, Iteration 766, loss = 0.2338\n",
            "Checking accuracy on validation set\n",
            "Got 371 / 1000 correct (37.10)\n",
            "\n",
            "Epoch 49, Iteration 766, loss = 0.5058\n",
            "Checking accuracy on validation set\n",
            "Got 381 / 1000 correct (38.10)\n",
            "\n",
            "Epoch 50, Iteration 766, loss = 0.2648\n",
            "Checking accuracy on validation set\n",
            "Got 376 / 1000 correct (37.60)\n",
            "\n",
            "Epoch 51, Iteration 766, loss = 0.3237\n",
            "Checking accuracy on validation set\n",
            "Got 371 / 1000 correct (37.10)\n",
            "\n",
            "Epoch 52, Iteration 766, loss = 0.2686\n",
            "Checking accuracy on validation set\n",
            "Got 378 / 1000 correct (37.80)\n",
            "\n",
            "Epoch 53, Iteration 766, loss = 0.2575\n",
            "Checking accuracy on validation set\n",
            "Got 352 / 1000 correct (35.20)\n",
            "\n",
            "Epoch 54, Iteration 766, loss = 0.1910\n",
            "Checking accuracy on validation set\n",
            "Got 360 / 1000 correct (36.00)\n",
            "\n",
            "Epoch 55, Iteration 766, loss = 0.1513\n",
            "Checking accuracy on validation set\n",
            "Got 371 / 1000 correct (37.10)\n",
            "\n",
            "Epoch 56, Iteration 766, loss = 0.1171\n",
            "Checking accuracy on validation set\n",
            "Got 368 / 1000 correct (36.80)\n",
            "\n",
            "Epoch 57, Iteration 766, loss = 0.4392\n",
            "Checking accuracy on validation set\n",
            "Got 347 / 1000 correct (34.70)\n",
            "\n",
            "Epoch 58, Iteration 766, loss = 0.6117\n",
            "Checking accuracy on validation set\n",
            "Got 365 / 1000 correct (36.50)\n",
            "\n",
            "Epoch 59, Iteration 766, loss = 0.2036\n",
            "Checking accuracy on validation set\n",
            "Got 364 / 1000 correct (36.40)\n",
            "\n",
            "Epoch 60, Iteration 766, loss = 1.5710\n",
            "Checking accuracy on validation set\n",
            "Got 342 / 1000 correct (34.20)\n",
            "\n",
            "Epoch 61, Iteration 766, loss = 0.1319\n",
            "Checking accuracy on validation set\n",
            "Got 373 / 1000 correct (37.30)\n",
            "\n",
            "Epoch 62, Iteration 766, loss = 0.1176\n",
            "Checking accuracy on validation set\n",
            "Got 348 / 1000 correct (34.80)\n",
            "\n",
            "Epoch 63, Iteration 766, loss = 0.4011\n",
            "Checking accuracy on validation set\n",
            "Got 369 / 1000 correct (36.90)\n",
            "\n",
            "Epoch 64, Iteration 766, loss = 0.2632\n",
            "Checking accuracy on validation set\n",
            "Got 352 / 1000 correct (35.20)\n",
            "\n",
            "Epoch 65, Iteration 766, loss = 0.4176\n",
            "Checking accuracy on validation set\n",
            "Got 363 / 1000 correct (36.30)\n",
            "\n",
            "Epoch 66, Iteration 766, loss = 0.3649\n",
            "Checking accuracy on validation set\n",
            "Got 391 / 1000 correct (39.10)\n",
            "\n",
            "Epoch 67, Iteration 766, loss = 0.1921\n",
            "Checking accuracy on validation set\n",
            "Got 378 / 1000 correct (37.80)\n",
            "\n",
            "Epoch 68, Iteration 766, loss = 0.3067\n",
            "Checking accuracy on validation set\n",
            "Got 383 / 1000 correct (38.30)\n",
            "\n",
            "Epoch 69, Iteration 766, loss = 0.3174\n",
            "Checking accuracy on validation set\n",
            "Got 375 / 1000 correct (37.50)\n",
            "\n",
            "Epoch 70, Iteration 766, loss = 0.1027\n",
            "Checking accuracy on validation set\n",
            "Got 374 / 1000 correct (37.40)\n",
            "\n",
            "Epoch 71, Iteration 766, loss = 0.3050\n",
            "Checking accuracy on validation set\n",
            "Got 352 / 1000 correct (35.20)\n",
            "\n",
            "Epoch 72, Iteration 766, loss = 0.3872\n",
            "Checking accuracy on validation set\n",
            "Got 345 / 1000 correct (34.50)\n",
            "\n",
            "Epoch 73, Iteration 766, loss = 0.7982\n",
            "Checking accuracy on validation set\n",
            "Got 356 / 1000 correct (35.60)\n",
            "\n",
            "Epoch 74, Iteration 766, loss = 0.1654\n",
            "Checking accuracy on validation set\n",
            "Got 373 / 1000 correct (37.30)\n",
            "\n",
            "Epoch 75, Iteration 766, loss = 0.1410\n",
            "Checking accuracy on validation set\n",
            "Got 368 / 1000 correct (36.80)\n",
            "\n",
            "Epoch 76, Iteration 766, loss = 0.4224\n",
            "Checking accuracy on validation set\n",
            "Got 365 / 1000 correct (36.50)\n",
            "\n",
            "Epoch 77, Iteration 766, loss = 0.5340\n",
            "Checking accuracy on validation set\n",
            "Got 388 / 1000 correct (38.80)\n",
            "\n",
            "Epoch 78, Iteration 766, loss = 0.0606\n",
            "Checking accuracy on validation set\n",
            "Got 383 / 1000 correct (38.30)\n",
            "\n",
            "Epoch 79, Iteration 766, loss = 0.3283\n",
            "Checking accuracy on validation set\n",
            "Got 366 / 1000 correct (36.60)\n",
            "\n",
            "Epoch 80, Iteration 766, loss = 0.2149\n",
            "Checking accuracy on validation set\n",
            "Got 369 / 1000 correct (36.90)\n",
            "\n",
            "Epoch 81, Iteration 766, loss = 0.2505\n",
            "Checking accuracy on validation set\n",
            "Got 372 / 1000 correct (37.20)\n",
            "\n",
            "Epoch 82, Iteration 766, loss = 0.4502\n",
            "Checking accuracy on validation set\n",
            "Got 383 / 1000 correct (38.30)\n",
            "\n",
            "Epoch 83, Iteration 766, loss = 0.6575\n",
            "Checking accuracy on validation set\n",
            "Got 359 / 1000 correct (35.90)\n",
            "\n",
            "Epoch 84, Iteration 766, loss = 0.1238\n",
            "Checking accuracy on validation set\n",
            "Got 370 / 1000 correct (37.00)\n",
            "\n",
            "Epoch 85, Iteration 766, loss = 0.4085\n",
            "Checking accuracy on validation set\n",
            "Got 361 / 1000 correct (36.10)\n",
            "\n",
            "Epoch 86, Iteration 766, loss = 0.0063\n",
            "Checking accuracy on validation set\n",
            "Got 367 / 1000 correct (36.70)\n",
            "\n",
            "Epoch 87, Iteration 766, loss = 0.1211\n",
            "Checking accuracy on validation set\n",
            "Got 347 / 1000 correct (34.70)\n",
            "\n",
            "Epoch 88, Iteration 766, loss = 0.2734\n",
            "Checking accuracy on validation set\n",
            "Got 354 / 1000 correct (35.40)\n",
            "\n",
            "Epoch 89, Iteration 766, loss = 0.1817\n",
            "Checking accuracy on validation set\n",
            "Got 374 / 1000 correct (37.40)\n",
            "\n",
            "Epoch 90, Iteration 766, loss = 0.3571\n",
            "Checking accuracy on validation set\n",
            "Got 385 / 1000 correct (38.50)\n",
            "\n",
            "Epoch 91, Iteration 766, loss = 0.2313\n",
            "Checking accuracy on validation set\n",
            "Got 367 / 1000 correct (36.70)\n",
            "\n",
            "Epoch 92, Iteration 766, loss = 0.2919\n",
            "Checking accuracy on validation set\n",
            "Got 346 / 1000 correct (34.60)\n",
            "\n",
            "Epoch 93, Iteration 766, loss = 0.4133\n",
            "Checking accuracy on validation set\n",
            "Got 353 / 1000 correct (35.30)\n",
            "\n",
            "Epoch 94, Iteration 766, loss = 0.0155\n",
            "Checking accuracy on validation set\n",
            "Got 372 / 1000 correct (37.20)\n",
            "\n",
            "Epoch 95, Iteration 766, loss = 0.2230\n",
            "Checking accuracy on validation set\n",
            "Got 373 / 1000 correct (37.30)\n",
            "\n",
            "Epoch 96, Iteration 766, loss = 0.2539\n",
            "Checking accuracy on validation set\n",
            "Got 367 / 1000 correct (36.70)\n",
            "\n",
            "Epoch 97, Iteration 766, loss = 0.1232\n",
            "Checking accuracy on validation set\n",
            "Got 383 / 1000 correct (38.30)\n",
            "\n",
            "Epoch 98, Iteration 766, loss = 0.0841\n",
            "Checking accuracy on validation set\n",
            "Got 385 / 1000 correct (38.50)\n",
            "\n",
            "Epoch 99, Iteration 766, loss = 0.5534\n",
            "Checking accuracy on validation set\n",
            "Got 353 / 1000 correct (35.30)\n",
            "\n",
            "Checking accuracy on validation set\n",
            "Got 353 / 1000 correct (35.30)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.353"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "learning_rate = 1e-3\n",
        "\n",
        "model = None\n",
        "optimizer = None\n",
        "\n",
        "model = ResNet(3, num_class)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "train_part34(model, optimizer, epochs=100)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1NshPI99sZq"
      },
      "source": [
        "## With BatchNorm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "D7__WkTu9sZr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a282347-7094-402c-9dfe-a9b186b7b1b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Iteration 766, loss = 3.0100\n",
            "Checking accuracy on validation set\n",
            "Got 302 / 1000 correct (30.20)\n",
            "\n",
            "Epoch 1, Iteration 766, loss = 2.6068\n",
            "Checking accuracy on validation set\n",
            "Got 398 / 1000 correct (39.80)\n",
            "\n",
            "Epoch 2, Iteration 766, loss = 2.1051\n",
            "Checking accuracy on validation set\n",
            "Got 431 / 1000 correct (43.10)\n",
            "\n",
            "Epoch 3, Iteration 766, loss = 1.5909\n",
            "Checking accuracy on validation set\n",
            "Got 483 / 1000 correct (48.30)\n",
            "\n",
            "Epoch 4, Iteration 766, loss = 1.5256\n",
            "Checking accuracy on validation set\n",
            "Got 512 / 1000 correct (51.20)\n",
            "\n",
            "Epoch 5, Iteration 766, loss = 1.1340\n",
            "Checking accuracy on validation set\n",
            "Got 509 / 1000 correct (50.90)\n",
            "\n",
            "Epoch 6, Iteration 766, loss = 0.6098\n",
            "Checking accuracy on validation set\n",
            "Got 502 / 1000 correct (50.20)\n",
            "\n",
            "Epoch 7, Iteration 766, loss = 0.8621\n",
            "Checking accuracy on validation set\n",
            "Got 508 / 1000 correct (50.80)\n",
            "\n",
            "Epoch 8, Iteration 766, loss = 0.4279\n",
            "Checking accuracy on validation set\n",
            "Got 537 / 1000 correct (53.70)\n",
            "\n",
            "Epoch 9, Iteration 766, loss = 0.3440\n",
            "Checking accuracy on validation set\n",
            "Got 518 / 1000 correct (51.80)\n",
            "\n",
            "Epoch 10, Iteration 766, loss = 0.3220\n",
            "Checking accuracy on validation set\n",
            "Got 522 / 1000 correct (52.20)\n",
            "\n",
            "Epoch 11, Iteration 766, loss = 0.3910\n",
            "Checking accuracy on validation set\n",
            "Got 523 / 1000 correct (52.30)\n",
            "\n",
            "Epoch 12, Iteration 766, loss = 0.2618\n",
            "Checking accuracy on validation set\n",
            "Got 510 / 1000 correct (51.00)\n",
            "\n",
            "Epoch 13, Iteration 766, loss = 0.2311\n",
            "Checking accuracy on validation set\n",
            "Got 505 / 1000 correct (50.50)\n",
            "\n",
            "Epoch 14, Iteration 766, loss = 0.2803\n",
            "Checking accuracy on validation set\n",
            "Got 495 / 1000 correct (49.50)\n",
            "\n",
            "Epoch 15, Iteration 766, loss = 0.0456\n",
            "Checking accuracy on validation set\n",
            "Got 511 / 1000 correct (51.10)\n",
            "\n",
            "Epoch 16, Iteration 766, loss = 0.2889\n",
            "Checking accuracy on validation set\n",
            "Got 521 / 1000 correct (52.10)\n",
            "\n",
            "Epoch 17, Iteration 766, loss = 0.1201\n",
            "Checking accuracy on validation set\n",
            "Got 502 / 1000 correct (50.20)\n",
            "\n",
            "Epoch 18, Iteration 766, loss = 0.2649\n",
            "Checking accuracy on validation set\n",
            "Got 510 / 1000 correct (51.00)\n",
            "\n",
            "Epoch 19, Iteration 766, loss = 0.0792\n",
            "Checking accuracy on validation set\n",
            "Got 523 / 1000 correct (52.30)\n",
            "\n",
            "Epoch 20, Iteration 766, loss = 0.0609\n",
            "Checking accuracy on validation set\n",
            "Got 502 / 1000 correct (50.20)\n",
            "\n",
            "Epoch 21, Iteration 766, loss = 0.0475\n",
            "Checking accuracy on validation set\n",
            "Got 522 / 1000 correct (52.20)\n",
            "\n",
            "Epoch 22, Iteration 766, loss = 0.1000\n",
            "Checking accuracy on validation set\n",
            "Got 521 / 1000 correct (52.10)\n",
            "\n",
            "Epoch 23, Iteration 766, loss = 0.2768\n",
            "Checking accuracy on validation set\n",
            "Got 530 / 1000 correct (53.00)\n",
            "\n",
            "Epoch 24, Iteration 766, loss = 0.0723\n",
            "Checking accuracy on validation set\n",
            "Got 516 / 1000 correct (51.60)\n",
            "\n",
            "Epoch 25, Iteration 766, loss = 0.0624\n",
            "Checking accuracy on validation set\n",
            "Got 521 / 1000 correct (52.10)\n",
            "\n",
            "Epoch 26, Iteration 766, loss = 0.0671\n",
            "Checking accuracy on validation set\n",
            "Got 517 / 1000 correct (51.70)\n",
            "\n",
            "Epoch 27, Iteration 766, loss = 0.1118\n",
            "Checking accuracy on validation set\n",
            "Got 512 / 1000 correct (51.20)\n",
            "\n",
            "Epoch 28, Iteration 766, loss = 0.0778\n",
            "Checking accuracy on validation set\n",
            "Got 512 / 1000 correct (51.20)\n",
            "\n",
            "Epoch 29, Iteration 766, loss = 0.0226\n",
            "Checking accuracy on validation set\n",
            "Got 518 / 1000 correct (51.80)\n",
            "\n",
            "Epoch 30, Iteration 766, loss = 0.1010\n",
            "Checking accuracy on validation set\n",
            "Got 509 / 1000 correct (50.90)\n",
            "\n",
            "Epoch 31, Iteration 766, loss = 0.1815\n",
            "Checking accuracy on validation set\n",
            "Got 495 / 1000 correct (49.50)\n",
            "\n",
            "Epoch 32, Iteration 766, loss = 0.0086\n",
            "Checking accuracy on validation set\n",
            "Got 516 / 1000 correct (51.60)\n",
            "\n",
            "Epoch 33, Iteration 766, loss = 0.0104\n",
            "Checking accuracy on validation set\n",
            "Got 520 / 1000 correct (52.00)\n",
            "\n",
            "Epoch 34, Iteration 766, loss = 0.0426\n",
            "Checking accuracy on validation set\n",
            "Got 537 / 1000 correct (53.70)\n",
            "\n",
            "Epoch 35, Iteration 766, loss = 0.0149\n",
            "Checking accuracy on validation set\n",
            "Got 522 / 1000 correct (52.20)\n",
            "\n",
            "Epoch 36, Iteration 766, loss = 0.0240\n",
            "Checking accuracy on validation set\n",
            "Got 518 / 1000 correct (51.80)\n",
            "\n",
            "Epoch 37, Iteration 766, loss = 0.0016\n",
            "Checking accuracy on validation set\n",
            "Got 508 / 1000 correct (50.80)\n",
            "\n",
            "Epoch 38, Iteration 766, loss = 0.1075\n",
            "Checking accuracy on validation set\n",
            "Got 510 / 1000 correct (51.00)\n",
            "\n",
            "Epoch 39, Iteration 766, loss = 0.0038\n",
            "Checking accuracy on validation set\n",
            "Got 507 / 1000 correct (50.70)\n",
            "\n",
            "Epoch 40, Iteration 766, loss = 0.0141\n",
            "Checking accuracy on validation set\n",
            "Got 493 / 1000 correct (49.30)\n",
            "\n",
            "Epoch 41, Iteration 766, loss = 0.1180\n",
            "Checking accuracy on validation set\n",
            "Got 528 / 1000 correct (52.80)\n",
            "\n",
            "Epoch 42, Iteration 766, loss = 0.0055\n",
            "Checking accuracy on validation set\n",
            "Got 508 / 1000 correct (50.80)\n",
            "\n",
            "Epoch 43, Iteration 766, loss = 0.2891\n",
            "Checking accuracy on validation set\n",
            "Got 492 / 1000 correct (49.20)\n",
            "\n",
            "Epoch 44, Iteration 766, loss = 0.0224\n",
            "Checking accuracy on validation set\n",
            "Got 530 / 1000 correct (53.00)\n",
            "\n",
            "Epoch 45, Iteration 766, loss = 0.0664\n",
            "Checking accuracy on validation set\n",
            "Got 480 / 1000 correct (48.00)\n",
            "\n",
            "Epoch 46, Iteration 766, loss = 0.0483\n",
            "Checking accuracy on validation set\n",
            "Got 497 / 1000 correct (49.70)\n",
            "\n",
            "Epoch 47, Iteration 766, loss = 0.0984\n",
            "Checking accuracy on validation set\n",
            "Got 500 / 1000 correct (50.00)\n",
            "\n",
            "Epoch 48, Iteration 766, loss = 0.0163\n",
            "Checking accuracy on validation set\n",
            "Got 527 / 1000 correct (52.70)\n",
            "\n",
            "Epoch 49, Iteration 766, loss = 0.0060\n",
            "Checking accuracy on validation set\n",
            "Got 515 / 1000 correct (51.50)\n",
            "\n",
            "Epoch 50, Iteration 766, loss = 0.0374\n",
            "Checking accuracy on validation set\n",
            "Got 512 / 1000 correct (51.20)\n",
            "\n",
            "Epoch 51, Iteration 766, loss = 0.1541\n",
            "Checking accuracy on validation set\n",
            "Got 522 / 1000 correct (52.20)\n",
            "\n",
            "Epoch 52, Iteration 766, loss = 0.0071\n",
            "Checking accuracy on validation set\n",
            "Got 492 / 1000 correct (49.20)\n",
            "\n",
            "Epoch 53, Iteration 766, loss = 0.0237\n",
            "Checking accuracy on validation set\n",
            "Got 520 / 1000 correct (52.00)\n",
            "\n",
            "Epoch 54, Iteration 766, loss = 0.0183\n",
            "Checking accuracy on validation set\n",
            "Got 505 / 1000 correct (50.50)\n",
            "\n",
            "Epoch 55, Iteration 766, loss = 0.0960\n",
            "Checking accuracy on validation set\n",
            "Got 512 / 1000 correct (51.20)\n",
            "\n",
            "Epoch 56, Iteration 766, loss = 0.0030\n",
            "Checking accuracy on validation set\n",
            "Got 526 / 1000 correct (52.60)\n",
            "\n",
            "Epoch 57, Iteration 766, loss = 0.0219\n",
            "Checking accuracy on validation set\n",
            "Got 509 / 1000 correct (50.90)\n",
            "\n",
            "Epoch 58, Iteration 766, loss = 0.0119\n",
            "Checking accuracy on validation set\n",
            "Got 499 / 1000 correct (49.90)\n",
            "\n",
            "Epoch 59, Iteration 766, loss = 0.0053\n",
            "Checking accuracy on validation set\n",
            "Got 493 / 1000 correct (49.30)\n",
            "\n",
            "Epoch 60, Iteration 766, loss = 0.0421\n",
            "Checking accuracy on validation set\n",
            "Got 508 / 1000 correct (50.80)\n",
            "\n",
            "Epoch 61, Iteration 766, loss = 0.0031\n",
            "Checking accuracy on validation set\n",
            "Got 528 / 1000 correct (52.80)\n",
            "\n",
            "Epoch 62, Iteration 766, loss = 0.0079\n",
            "Checking accuracy on validation set\n",
            "Got 525 / 1000 correct (52.50)\n",
            "\n",
            "Epoch 63, Iteration 766, loss = 0.0136\n",
            "Checking accuracy on validation set\n",
            "Got 516 / 1000 correct (51.60)\n",
            "\n",
            "Epoch 64, Iteration 766, loss = 0.0364\n",
            "Checking accuracy on validation set\n",
            "Got 516 / 1000 correct (51.60)\n",
            "\n",
            "Epoch 65, Iteration 766, loss = 0.0777\n",
            "Checking accuracy on validation set\n",
            "Got 506 / 1000 correct (50.60)\n",
            "\n",
            "Epoch 66, Iteration 766, loss = 0.0316\n",
            "Checking accuracy on validation set\n",
            "Got 513 / 1000 correct (51.30)\n",
            "\n",
            "Epoch 67, Iteration 766, loss = 0.0378\n",
            "Checking accuracy on validation set\n",
            "Got 501 / 1000 correct (50.10)\n",
            "\n",
            "Epoch 68, Iteration 766, loss = 0.0277\n",
            "Checking accuracy on validation set\n",
            "Got 511 / 1000 correct (51.10)\n",
            "\n",
            "Epoch 69, Iteration 766, loss = 0.0009\n",
            "Checking accuracy on validation set\n",
            "Got 523 / 1000 correct (52.30)\n",
            "\n",
            "Epoch 70, Iteration 766, loss = 0.0037\n",
            "Checking accuracy on validation set\n",
            "Got 530 / 1000 correct (53.00)\n",
            "\n",
            "Epoch 71, Iteration 766, loss = 0.0167\n",
            "Checking accuracy on validation set\n",
            "Got 513 / 1000 correct (51.30)\n",
            "\n",
            "Epoch 72, Iteration 766, loss = 0.0156\n",
            "Checking accuracy on validation set\n",
            "Got 513 / 1000 correct (51.30)\n",
            "\n",
            "Epoch 73, Iteration 766, loss = 0.0060\n",
            "Checking accuracy on validation set\n",
            "Got 522 / 1000 correct (52.20)\n",
            "\n",
            "Epoch 74, Iteration 766, loss = 0.0662\n",
            "Checking accuracy on validation set\n",
            "Got 516 / 1000 correct (51.60)\n",
            "\n",
            "Epoch 75, Iteration 766, loss = 0.0437\n",
            "Checking accuracy on validation set\n",
            "Got 508 / 1000 correct (50.80)\n",
            "\n",
            "Epoch 76, Iteration 766, loss = 0.0918\n",
            "Checking accuracy on validation set\n",
            "Got 499 / 1000 correct (49.90)\n",
            "\n",
            "Epoch 77, Iteration 766, loss = 0.1514\n",
            "Checking accuracy on validation set\n",
            "Got 519 / 1000 correct (51.90)\n",
            "\n",
            "Epoch 78, Iteration 766, loss = 0.0040\n",
            "Checking accuracy on validation set\n",
            "Got 526 / 1000 correct (52.60)\n",
            "\n",
            "Epoch 79, Iteration 766, loss = 0.0063\n",
            "Checking accuracy on validation set\n",
            "Got 513 / 1000 correct (51.30)\n",
            "\n",
            "Epoch 80, Iteration 766, loss = 0.0267\n",
            "Checking accuracy on validation set\n",
            "Got 495 / 1000 correct (49.50)\n",
            "\n",
            "Epoch 81, Iteration 766, loss = 0.0180\n",
            "Checking accuracy on validation set\n",
            "Got 515 / 1000 correct (51.50)\n",
            "\n",
            "Epoch 82, Iteration 766, loss = 0.0367\n",
            "Checking accuracy on validation set\n",
            "Got 480 / 1000 correct (48.00)\n",
            "\n",
            "Epoch 83, Iteration 766, loss = 0.0155\n",
            "Checking accuracy on validation set\n",
            "Got 502 / 1000 correct (50.20)\n",
            "\n",
            "Epoch 84, Iteration 766, loss = 0.0014\n",
            "Checking accuracy on validation set\n",
            "Got 486 / 1000 correct (48.60)\n",
            "\n",
            "Epoch 85, Iteration 766, loss = 0.0201\n",
            "Checking accuracy on validation set\n",
            "Got 489 / 1000 correct (48.90)\n",
            "\n",
            "Epoch 86, Iteration 766, loss = 0.0136\n",
            "Checking accuracy on validation set\n",
            "Got 504 / 1000 correct (50.40)\n",
            "\n",
            "Epoch 87, Iteration 766, loss = 0.0003\n",
            "Checking accuracy on validation set\n",
            "Got 504 / 1000 correct (50.40)\n",
            "\n",
            "Epoch 88, Iteration 766, loss = 0.0140\n",
            "Checking accuracy on validation set\n",
            "Got 499 / 1000 correct (49.90)\n",
            "\n",
            "Epoch 89, Iteration 766, loss = 0.1279\n",
            "Checking accuracy on validation set\n",
            "Got 520 / 1000 correct (52.00)\n",
            "\n",
            "Epoch 90, Iteration 766, loss = 0.0052\n",
            "Checking accuracy on validation set\n",
            "Got 504 / 1000 correct (50.40)\n",
            "\n",
            "Epoch 91, Iteration 766, loss = 0.0459\n",
            "Checking accuracy on validation set\n",
            "Got 528 / 1000 correct (52.80)\n",
            "\n",
            "Epoch 92, Iteration 766, loss = 0.1778\n",
            "Checking accuracy on validation set\n",
            "Got 504 / 1000 correct (50.40)\n",
            "\n",
            "Epoch 93, Iteration 766, loss = 0.0221\n",
            "Checking accuracy on validation set\n",
            "Got 500 / 1000 correct (50.00)\n",
            "\n",
            "Epoch 94, Iteration 766, loss = 0.0027\n",
            "Checking accuracy on validation set\n",
            "Got 517 / 1000 correct (51.70)\n",
            "\n",
            "Epoch 95, Iteration 766, loss = 0.0005\n",
            "Checking accuracy on validation set\n",
            "Got 515 / 1000 correct (51.50)\n",
            "\n",
            "Epoch 96, Iteration 766, loss = 0.0067\n",
            "Checking accuracy on validation set\n",
            "Got 521 / 1000 correct (52.10)\n",
            "\n",
            "Epoch 97, Iteration 766, loss = 0.0882\n",
            "Checking accuracy on validation set\n",
            "Got 474 / 1000 correct (47.40)\n",
            "\n",
            "Epoch 98, Iteration 766, loss = 0.0018\n",
            "Checking accuracy on validation set\n",
            "Got 504 / 1000 correct (50.40)\n",
            "\n",
            "Epoch 99, Iteration 766, loss = 0.0014\n",
            "Checking accuracy on validation set\n",
            "Got 498 / 1000 correct (49.80)\n",
            "\n",
            "Checking accuracy on validation set\n",
            "Got 498 / 1000 correct (49.80)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.498"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "learning_rate = 1e-3\n",
        "\n",
        "model = None\n",
        "optimizer = None\n",
        "\n",
        "################################################################################\n",
        "# TODO: InstantiateResnet with BatchNorm                                       #\n",
        "################################################################################\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "model = ResNet(3, num_class, batch_use = True)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "################################################################################\n",
        "#                                 END OF YOUR CODE\n",
        "################################################################################\n",
        "\n",
        "\n",
        "train_part34(model, optimizer, epochs=100)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUmXvJdU9sZr"
      },
      "source": [
        "## Discussion on BatchNorm\n",
        "### TODO: Write your answer here\n",
        "### 1.Validation Accuracy:\n",
        "#### Without BatchNorm :\n",
        "Starts at 8.00% (Epoch 0) and improves steadily to 40.00% (Epoch 9), with a final validation accuracy of 39.40%.\n",
        "The accuracy increases consistently but plateaus around 36-40% after Epoch 6, indicating limited further improvement.\n",
        "#### With BatchNorm :\n",
        "Starts at 25.90% (Epoch 0) and reaches 51.20% (Epoch 9), with a final validation accuracy of 51.00%.\n",
        "The accuracy improves more rapidly and achieves significantly higher values, surpassing 50% by Epoch 6 and stabilizing around 50-51% in later epochs.\n",
        "\n",
        "### 2.Convergence Speed :\n",
        "#### Without BatchNorm :\n",
        "The model improves slowly, taking until Epoch 9 to reach 40.00% accuracy. The loss reduction is less stable, and accuracy plateaus earlier.\n",
        "#### With BatchNorm :\n",
        "The model converges faster, reaching 41.60% accuracy by Epoch 2 (vs. 22.90% without BatchNorm) and continuing to improve until Epoch 9. The loss decreases more smoothly and rapidly.\n",
        "### Summary of Performance :\n",
        "With BatchNorm: Achieves higher validation accuracy (51.00% vs. 39.40%), lower training loss (0.3992 vs. 1.0206), faster convergence, and greater training stability.\n",
        "\n",
        "Without BatchNorm: Performs worse, with slower convergence, higher loss, and lower final accuracy, plateauing at a lower performance level."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cbjkr0H99sZr"
      },
      "source": [
        "## Discuss effect of Batch Size\n",
        "### TODO: Write your answer here\n",
        "#### the effect of different batch sizes\n",
        "1. As batch size increases from 32 to 64 and 128, validation accuracy decreases (53.40% > 49.30% ≈ 49.90%). Batch size 32 clearly outperforms larger batch sizes in both final accuracy and stability.\n",
        "2. Batch size 32 converges steadily and maintains improvement. Batch size 64 converges quickly early but stalls with fluctuations. Batch size 128’s converges the fastest.\n",
        "\n",
        "#### possible reasons\n",
        "1. As batch size increases from 32 to 64 and 128, validation accuracy decreases (53.40% > 49.30% ≈ 49.90%) because larger batch sizes lead to more stable but less noisy gradients, potentially causing the model to converge to sharper minima with poorer generalization. Batch size 32 introduces more stochasticity, helping the model escape local minima and achieve better generalization, resulting in higher accuracy and stability.\n",
        "2. Batch size 32 converges steadily due to balanced gradient noise, while 64 converges quickly but fluctuates as noise decreases, causing instability. Batch size 128 converges fastest with the most stable gradients but may overfit, limiting further improvement.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmBrX3D89sZr"
      },
      "source": [
        "# CIFAR-100 open-ended challenge\n",
        "## What I did\n",
        "\n",
        "In the cell below you should write an explanation of what you did, any additional features that you implemented, and/or any graphs that you made in the process of training and evaluating your network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZ1ttqnk9sZs",
        "outputId": "582131db-d41b-41fb-debe-05ca4a4972f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Iteration 1532, loss = 3.6260\n",
            "Checking accuracy on validation set\n",
            "Got 93 / 1000 correct (9.30)\n",
            "\n",
            "Epoch 1, Iteration 1532, loss = 4.1110\n",
            "Checking accuracy on validation set\n",
            "Got 186 / 1000 correct (18.60)\n",
            "\n",
            "Epoch 2, Iteration 1532, loss = 2.9544\n",
            "Checking accuracy on validation set\n",
            "Got 278 / 1000 correct (27.80)\n",
            "\n",
            "Epoch 3, Iteration 1532, loss = 3.3307\n",
            "Checking accuracy on validation set\n",
            "Got 330 / 1000 correct (33.00)\n",
            "\n",
            "Epoch 4, Iteration 1532, loss = 3.8524\n",
            "Checking accuracy on validation set\n",
            "Got 392 / 1000 correct (39.20)\n",
            "\n",
            "Epoch 5, Iteration 1532, loss = 3.1999\n",
            "Checking accuracy on validation set\n",
            "Got 424 / 1000 correct (42.40)\n",
            "\n",
            "Epoch 6, Iteration 1532, loss = 2.4103\n",
            "Checking accuracy on validation set\n",
            "Got 484 / 1000 correct (48.40)\n",
            "\n",
            "Epoch 7, Iteration 1532, loss = 1.8405\n",
            "Checking accuracy on validation set\n",
            "Got 499 / 1000 correct (49.90)\n",
            "\n",
            "Epoch 8, Iteration 1532, loss = 0.6801\n",
            "Checking accuracy on validation set\n",
            "Got 531 / 1000 correct (53.10)\n",
            "\n",
            "Epoch 9, Iteration 1532, loss = 2.2312\n",
            "Checking accuracy on validation set\n",
            "Got 528 / 1000 correct (52.80)\n",
            "\n",
            "Checking accuracy on validation set\n",
            "Got 528 / 1000 correct (52.80)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.528"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "\n",
        "model = None\n",
        "optimizer = None\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, batch_use = False):\n",
        "        super(BasicBlock, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, (3,3), stride=1, padding=1, bias = True)\n",
        "        # only use Batchnorm in part 2\n",
        "        self.BatchNorm1 = nn.BatchNorm2d(out_channels) if batch_use else nn.Identity()\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, (3,3), stride=1, padding=1, bias = True)\n",
        "        self.BatchNorm2 = nn.BatchNorm2d(out_channels) if batch_use else nn.Identity()\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.conv1.weight.data = random_weight((out_channels, in_channels, 3, 3))\n",
        "        self.conv1.bias.data = zero_weight((out_channels,))\n",
        "        self.conv2.weight.data = random_weight((out_channels, out_channels, 3, 3))\n",
        "        self.conv2.bias.data = zero_weight((out_channels,))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.BatchNorm1(out)\n",
        "        out = self.relu1(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.BatchNorm2(out)\n",
        "        out = self.relu2(out)\n",
        "        return out\n",
        "\n",
        "class NewDefinedNet(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, num_classes = 100, batch_use = False):\n",
        "        super(NewDefinedNet, self).__init__()\n",
        "\n",
        "        # conv1\n",
        "        self.conv1 = nn.Conv2d(in_channels, 64, (7,7), stride=2, padding=1, bias = True)\n",
        "        self.BatchNorm = nn.BatchNorm2d(64) if batch_use else nn.Identity()\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d((3,3), stride=2, padding=1)\n",
        "\n",
        "        # conv2_x\n",
        "\n",
        "        self.conv2_x = nn.Sequential(\n",
        "            BasicBlock(64, 256, batch_use),\n",
        "            nn.Dropout(p=0.5),\n",
        "            BasicBlock(256, 512, batch_use),\n",
        "        )\n",
        "        # conv3_3\n",
        "        self.conv3_x = nn.Sequential(\n",
        "            BasicBlock(512, 1024, batch_use),\n",
        "            nn.Dropout(p=0.5),\n",
        "            BasicBlock(1024, 2048, batch_use),\n",
        "        )\n",
        "\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        # fc\n",
        "        self.fc = nn.Linear(2048, num_classes)\n",
        "\n",
        "        self.conv1.weight.data = random_weight((64, in_channels, 7, 7))\n",
        "        self.conv1.bias.data = zero_weight((64,))\n",
        "        self.fc.weight.data = random_weight((num_classes, 2048))\n",
        "        self.fc.bias.data = zero_weight((num_classes,))\n",
        "    def forward(self,x):\n",
        "        x = self.conv1(x)\n",
        "\n",
        "        x = self.BatchNorm(x)\n",
        "        x = self.relu(x)\n",
        "        # x = self.maxpool(x)\n",
        "        x = self.conv2_x(x)\n",
        "        x = self.conv3_x(x)\n",
        "        x = self.avg_pool(x)\n",
        "        x = flatten(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "batch_size = 32\n",
        "learning_rate = 1e-3\n",
        "\n",
        "\n",
        "\n",
        "loader_train = DataLoader(cifar100_train, batch_size=batch_size, num_workers=2,\n",
        "                          sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
        "\n",
        "loader_val = DataLoader(cifar100_val, batch_size=batch_size, num_workers=2,\n",
        "                        sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, 50000)))\n",
        "\n",
        "loader_test = DataLoader(cifar100_test, batch_size=batch_size, num_workers=2)\n",
        "\n",
        "\n",
        "model = NewDefinedNet(3, num_class, batch_use = True)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_part34(model, optimizer, epochs=10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQ5gncXk9sZs"
      },
      "source": [
        "TODO: Describe what you did\n",
        "\n",
        "I implemented a custom neural network (NewDefinedNet) for CIFAR-10 classification, using a batch size of 32, a learning rate of 10^-3 , and the Adam optimizer. The network structure includes a 7x7 convolutional layer (conv1) followed by batch normalization (optional), ReLU activation, and a max-pooling layer (commented out for experimentation). It then uses two sequential blocks (conv2_x and conv3_x), each containing two BasicBlock units with increasing channels (64→256→512 and 512→1024→2048) and dropout (p=0.5) for regularization. Additional features include flexible batch normalization via the batch_use flag and random_weight, zero_weight to initialize training process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaBGAJck7PsS"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mm26j2Wf9sZs"
      },
      "source": [
        "## Test set -- run this only once\n",
        "\n",
        "Now that we've gotten a result we're happy with, we test our final model on the test set (which you should store in best_model). Think about how this compares to your validation set accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "yOVVJvU69sZs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46965673-da79-448d-e0bb-8d67f2414ec5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking accuracy on test set\n",
            "Got 5301 / 10000 correct (53.01)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5301"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "best_model = model\n",
        "check_accuracy_part34(loader_test, best_model)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    },
    "toc": {
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}